\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{fontawesome5}
\usetikzlibrary{shapes,arrows,positioning}

% Code listing style
\lstset{
    basicstyle=\ttfamily\footnotesize,
    keywordstyle=\color{blue},
    commentstyle=\color{green!50!black},
    stringstyle=\color{red},
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10}
}

% Title page
\title{3D Bounding Box Detection System}
\subtitle{Real-Time Object Detection and 3D Localization using RGB-D Cameras}
\author{HUOJIAXI}
\institute{ROS 2 Humble + YOLO + Point Cloud Processing}
\date{\today}

\begin{document}

% Title slide
\begin{frame}
\titlepage
\end{frame}

% Table of contents
\begin{frame}{Outline}
\tableofcontents
\end{frame}

% Section 1: Introduction
\section{Introduction}

\begin{frame}{Project Overview}
\begin{columns}
\column{0.5\textwidth}
\textbf{Objective:}
\begin{itemize}
    \item Detect objects in 3D space
    \item Provide real-world coordinates
    \item Real-time performance
    \item ROS 2 integration
\end{itemize}

\vspace{1em}
\textbf{Applications:}
\begin{itemize}
    \item Robotic manipulation
    \item Autonomous navigation
    \item Object tracking
    \item Human-robot interaction
\end{itemize}

\column{0.5\textwidth}
\begin{center}
\begin{tikzpicture}[scale=0.8]
    % Camera
    \draw[fill=blue!20] (0,3) rectangle (2,4);
    \node at (1,3.5) {\small RGB-D Camera};

    % Arrow down
    \draw[->,thick] (1,3) -- (1,2.2);

    % Detection box
    \draw[fill=green!20] (-0.5,1) rectangle (2.5,2);
    \node at (1,1.5) {\small 2D Detection};

    % Arrow down
    \draw[->,thick] (1,1) -- (1,0.2);

    % 3D box
    \draw[fill=orange!20] (-0.5,-1) rectangle (2.5,0);
    \node at (1,-0.5) {\small 3D Localization};
\end{tikzpicture}
\end{center}
\end{columns}
\end{frame}

\begin{frame}{System Architecture}
\begin{center}
\begin{tikzpicture}[
    node distance=1.5cm,
    block/.style={rectangle, draw, fill=blue!20, text width=3cm, text centered, rounded corners, minimum height=1cm},
    data/.style={rectangle, draw, fill=green!20, text width=2.5cm, text centered, rounded corners, minimum height=0.8cm},
    arrow/.style={->, thick}
]
    % Input layer
    \node[data] (camera) {RGB-D Camera};
    \node[data, right=of camera] (rgb) {RGB Image};
    \node[data, right=of rgb] (depth) {Point Cloud};

    % Processing layer
    \node[block, below=of rgb] (darknet) {darknet\_ros\\(YOLO)};
    \node[data, below=of darknet] (bbox2d) {2D Boxes};

    % 3D processing
    \node[block, below=of bbox2d] (darknet3d) {darknet\_ros\_3d};
    \node[data, left=of darknet3d] (pc) {Depth Data};

    % Output
    \node[data, below=of darknet3d] (bbox3d) {3D Bounding Boxes};

    % Arrows
    \draw[arrow] (camera) -- (rgb);
    \draw[arrow] (camera) -- (depth);
    \draw[arrow] (rgb) -- (darknet);
    \draw[arrow] (darknet) -- (bbox2d);
    \draw[arrow] (bbox2d) -- (darknet3d);
    \draw[arrow] (depth) |- (pc);
    \draw[arrow] (pc) -- (darknet3d);
    \draw[arrow] (darknet3d) -- (bbox3d);
\end{tikzpicture}
\end{center}
\end{frame}

% Section 2: Technical Components
\section{Technical Components}

\begin{frame}{Component 1: darknet\_ros}
\textbf{Purpose:} 2D Object Detection using YOLO

\begin{columns}
\column{0.5\textwidth}
\textbf{Features:}
\begin{itemize}
    \item Real-time object detection
    \item 80+ object classes (COCO)
    \item GPU acceleration support
    \item ROS 2 Humble compatible
\end{itemize}

\vspace{0.5em}
\textbf{Input:}
\begin{itemize}
    \item RGB images from camera
\end{itemize}

\vspace{0.5em}
\textbf{Output:}
\begin{itemize}
    \item 2D bounding boxes
    \item Object class labels
    \item Confidence scores
\end{itemize}

\column{0.5\textwidth}
\textbf{Configuration:}
\lstinputlisting[language=yaml, basicstyle=\tiny\ttfamily]{src/darknet_ros/darknet_ros/config/ros.yaml}
\end{columns}
\end{frame}

\begin{frame}{Component 2: gb\_visual\_detection\_3d}
\textbf{Purpose:} Convert 2D detections to 3D bounding boxes

\begin{columns}
\column{0.5\textwidth}
\textbf{Algorithm:}
\begin{enumerate}
    \item Receive 2D bounding boxes
    \item Map pixel coordinates to 3D points
    \item Extract point cloud within 2D box
    \item Calculate min/max in X, Y, Z
    \item Filter outliers by depth
    \item Publish 3D bounding boxes
\end{enumerate}

\column{0.5\textwidth}
\textbf{Key Parameters:}
\begin{itemize}
    \item \texttt{point\_cloud\_topic}:\\
    \texttt{/camera/depth/points}
    \item \texttt{working\_frame}:\\
    \texttt{camera\_link}
    \item \texttt{max\_detection\_threshold}:\\
    0.8 meters
    \item \texttt{min\_probability}:\\
    0.1 (10\% confidence)
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{3D Projection Algorithm}
\textbf{Step-by-Step Process:}

\begin{enumerate}
    \item \textbf{Center Point Calculation:}
    \begin{equation*}
    (c_x, c_y) = \left(\frac{x_{min} + x_{max}}{2}, \frac{y_{min} + y_{max}}{2}\right)
    \end{equation*}

    \item \textbf{Point Cloud Indexing:}
    \begin{equation*}
    \text{index} = c_y \times \text{width} + c_x
    \end{equation*}

    \item \textbf{3D Bounds Computation:}
    \begin{equation*}
    \begin{aligned}
    X_{min/max} &= \min/\max(P_x) \quad \forall P \in \text{BBox2D} \\
    Y_{min/max} &= \min/\max(P_y) \quad \forall P \in \text{BBox2D} \\
    Z_{min/max} &= \min/\max(P_z) \quad \forall P \in \text{BBox2D}
    \end{aligned}
    \end{equation*}

    \item \textbf{Outlier Filtering:}
    \begin{equation*}
    |P_x - P_{center\_x}| < \text{threshold}
    \end{equation*}
\end{enumerate}
\end{frame}

% Section 3: Building Process
\section{Building Process}

\begin{frame}[fragile]{Step 1: Clone Repository}
\begin{lstlisting}[language=bash]
# Clone the repository
git clone git@github.com:HUOJIAXI/\
3D_bounding_box_gb_visual_detection.git gb_ws
cd gb_ws
\end{lstlisting}

\vspace{1em}
\textbf{Repository Structure:}
\begin{lstlisting}[basicstyle=\tiny\ttfamily]
gb_ws/
├── README.md
├── run_3d_detection.sh
├── download_weights.py
└── src/
    ├── darknet_ros/
    ├── gb_visual_detection_3d/
    └── gb_visual_detection_3d_msgs/
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Step 2: Install Dependencies}
\begin{lstlisting}[language=bash]
# ROS 2 dependencies
sudo apt update
sudo apt install -y \
    ros-humble-cv-bridge \
    ros-humble-vision-opencv \
    ros-humble-image-transport \
    ros-humble-tf2-ros \
    ros-humble-tf2-sensor-msgs \
    ros-humble-visualization-msgs

# System dependencies
sudo apt install -y \
    libopencv-dev \
    libeigen3-dev
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Step 3: Download YOLO Weights}
\begin{lstlisting}[language=bash]
# Option 1: Use the provided script
python3 download_weights.py

# Option 2: Manual download
cd src/darknet_ros/darknet_ros/\
yolo_network_config/weights/
wget https://pjreddie.com/media/files/yolov3.weights
\end{lstlisting}

\vspace{1em}
\textbf{Available Models:}
\begin{itemize}
    \item YOLOv3 (most accurate, slower)
    \item YOLOv3-tiny (faster, less accurate)
    \item YOLOv2 variants
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Step 4: Build the Workspace}
\begin{lstlisting}[language=bash]
# Source ROS 2
source /opt/ros/humble/setup.bash

# Build packages
colcon build --symlink-install

# Source the workspace
source install/setup.bash
\end{lstlisting}

\vspace{1em}
\textbf{Build Output:}
\begin{itemize}
    \item \texttt{gb\_visual\_detection\_3d\_msgs} - Message definitions
    \item \texttt{darknet\_ros\_3d} - 3D detection node
    \item \texttt{darknet\_ros} - 2D detection node (already built)
\end{itemize}
\end{frame}

% Section 4: Configuration
\section{Configuration}

\begin{frame}[fragile]{Camera Topic Configuration}
\textbf{File:} \texttt{src/darknet\_ros/darknet\_ros/config/ros.yaml}

\begin{lstlisting}[language=yaml, basicstyle=\tiny\ttfamily]
darknet_ros:
  ros__parameters:
    subscribers:
      camera_reading:
        topic: /camera/color/image_raw  # RGB image topic
        queue_size: 1
\end{lstlisting}

\vspace{1em}
\textbf{File:} \texttt{src/gb\_visual\_detection\_3d/darknet\_ros\_3d/config/darknet\_3d.yaml}

\begin{lstlisting}[language=yaml, basicstyle=\tiny\ttfamily]
darknet3d_node:
  ros__parameters:
    point_cloud_topic: /camera/depth/points
    working_frame: camera_link
    maximum_detection_threshold: 0.8
    minimum_probability: 0.1
    interested_classes: ["person", "book", "bottle"]
\end{lstlisting}
\end{frame}

\begin{frame}{Parameter Tuning Guide}
\begin{table}
\begin{tabular}{|l|p{5cm}|l|}
\hline
\textbf{Parameter} & \textbf{Description} & \textbf{Range} \\
\hline
\texttt{maximum\_detection\_threshold} & Max depth variation within object & 0.3-1.5m \\
\hline
\texttt{minimum\_probability} & Min confidence for detection & 0.1-0.9 \\
\hline
\texttt{interested\_classes} & Filter specific objects & Any COCO class \\
\hline
\texttt{working\_frame} & TF frame for coordinates & camera\_link \\
\hline
\end{tabular}
\end{table}

\vspace{1em}
\textbf{Recommendations:}
\begin{itemize}
    \item Lower threshold $\rightarrow$ More complete boxes, more noise
    \item Higher probability $\rightarrow$ Fewer false positives
    \item Limit classes $\rightarrow$ Better performance
\end{itemize}
\end{frame}

% Section 5: Running the System
\section{Running the System}

\begin{frame}[fragile]{Quick Start Method}
\textbf{Single Command Launch:}

\begin{lstlisting}[language=bash]
# Start camera driver first (example: RealSense)
ros2 launch realsense2_camera rs_launch.py \
    enable_depth:=true \
    enable_color:=true \
    pointcloud.enable:=true

# In another terminal, run the complete system
cd ~/gb_ws
source install/setup.bash
./run_3d_detection.sh
\end{lstlisting}

\vspace{1em}
This launches both:
\begin{itemize}
    \item darknet\_ros (2D detection)
    \item darknet\_ros\_3d (3D localization)
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Alternative: Launch File}
\begin{lstlisting}[language=bash]
# Using ROS 2 launch file
source install/setup.bash
ros2 launch darknet_ros_3d \
    darknet_ros_3d_complete.launch.py
\end{lstlisting}

\vspace{1em}
\textbf{Or launch components separately:}

\begin{lstlisting}[language=bash]
# Terminal 1: 2D detection
ros2 launch darknet_ros darknet_ros.launch.py

# Terminal 2: 3D detection
ros2 launch darknet_ros_3d darknet_ros_3d.launch.py
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Visualization in RViz2}
\begin{lstlisting}[language=bash]
# Launch RViz2
rviz2
\end{lstlisting}

\vspace{1em}
\textbf{RViz Configuration:}
\begin{enumerate}
    \item Set \textbf{Fixed Frame} to: \texttt{camera\_link}
    \item Add \textbf{Image} display:
    \begin{itemize}
        \item Topic: \texttt{/darknet\_ros/detection\_image}
    \end{itemize}
    \item Add \textbf{PointCloud2} display:
    \begin{itemize}
        \item Topic: \texttt{/camera/depth/points}
    \end{itemize}
    \item Add \textbf{MarkerArray} display:
    \begin{itemize}
        \item Topic: \texttt{/darknet\_ros\_3d/markers}
    \end{itemize}
\end{enumerate}
\end{frame}

% Section 6: ROS 2 Topics
\section{ROS 2 Interface}

\begin{frame}{Published Topics}
\begin{table}
\scriptsize
\begin{tabular}{|l|l|p{4cm}|}
\hline
\textbf{Topic} & \textbf{Type} & \textbf{Description} \\
\hline
\texttt{/darknet\_ros/bounding\_boxes} & BoundingBoxes & 2D detection results \\
\hline
\texttt{/darknet\_ros/detection\_image} & Image & Annotated RGB image \\
\hline
\texttt{/darknet\_ros\_3d/bounding\_boxes} & BoundingBoxes3d & 3D detection results \\
\hline
\texttt{/darknet\_ros\_3d/markers} & MarkerArray & RViz visualization \\
\hline
\end{tabular}
\end{table}

\vspace{1em}
\textbf{Subscribed Topics:}
\begin{table}
\scriptsize
\begin{tabular}{|l|l|}
\hline
\textbf{Topic} & \textbf{Type} \\
\hline
\texttt{/camera/color/image\_raw} & Image \\
\hline
\texttt{/camera/depth/points} & PointCloud2 \\
\hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]{Message Format: BoundingBox3d}
\begin{lstlisting}[language=bash]
# Message definition
string object_name       # Object class (e.g., "person")
float64 probability      # Confidence [0-1]
float64 xmin, xmax      # X-axis bounds (meters)
float64 ymin, ymax      # Y-axis bounds (meters)
float64 zmin, zmax      # Z-axis bounds (meters)
\end{lstlisting}

\vspace{1em}
\textbf{Example Output:}
\begin{lstlisting}[basicstyle=\tiny\ttfamily]
object_name: "person"
probability: 0.94
xmin: 0.45, xmax: 0.85  # 40cm width
ymin: -0.20, ymax: 0.20 # 40cm horizontal
zmin: 0.10, zmax: 1.75  # 165cm height
\end{lstlisting}
\end{frame}

\begin{frame}[fragile]{Monitoring and Debugging}
\textbf{Check Topic Activity:}
\begin{lstlisting}[language=bash]
# List all topics
ros2 topic list

# Check publishing rate
ros2 topic hz /camera/color/image_raw
ros2 topic hz /darknet_ros_3d/bounding_boxes

# View topic data
ros2 topic echo /darknet_ros_3d/bounding_boxes
\end{lstlisting}

\vspace{1em}
\textbf{Verify TF Transforms:}
\begin{lstlisting}[language=bash]
# View TF tree
ros2 run tf2_tools view_frames

# Check specific transform
ros2 run tf2_ros tf2_echo base_link camera_link
\end{lstlisting}
\end{frame}

% Section 7: Results and Performance
\section{Results and Performance}

\begin{frame}{System Performance}
\begin{columns}
\column{0.5\textwidth}
\textbf{Detection Accuracy:}
\begin{itemize}
    \item 2D Detection: 70-95\% (YOLO)
    \item 3D Localization: $\pm$5cm
    \item Range: 0.5m - 5m
    \item Classes: 80+ objects
\end{itemize}

\vspace{1em}
\textbf{Processing Speed:}
\begin{itemize}
    \item YOLOv3: 20-30 FPS (GPU)
    \item YOLOv3-tiny: 50-100 FPS (GPU)
    \item 3D Processing: $<$5ms overhead
\end{itemize}

\column{0.5\textwidth}
\textbf{Hardware Requirements:}
\begin{itemize}
    \item CPU: 4+ cores recommended
    \item RAM: 4GB minimum
    \item GPU: NVIDIA (optional, recommended)
    \item Camera: RGB-D with ROS driver
\end{itemize}

\vspace{1em}
\textbf{Supported Cameras:}
\begin{itemize}
    \item Intel RealSense D435/D455
    \item Asus Xtion Pro
    \item Orbbec Astra
    \item Any with PointCloud2 output
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Coordinate System}
\begin{center}
\begin{tikzpicture}[scale=1.2]
    % Camera
    \draw[fill=blue!20] (0,0) circle (0.3);
    \node at (0,0) {\tiny Cam};

    % Axes
    \draw[->,red,thick] (0,0) -- (3,0) node[right] {X (depth)};
    \draw[->,green!70!black,thick] (0,0) -- (0,3) node[above] {Z (height)};
    \draw[->,blue,thick] (0,0) -- (-1.5,-1) node[below] {Y (horizontal)};

    % Object
    \draw[thick,orange,fill=orange!20] (1.5,0.5) rectangle (2.5,2.5);
    \node at (2,1.5) {\small Person};

    % Dimensions
    \draw[<->,thick] (1.5,-0.3) -- (2.5,-0.3) node[midway,below] {\tiny $\Delta X$};
    \draw[<->,thick] (2.7,0.5) -- (2.7,2.5) node[midway,right] {\tiny $\Delta Z$};

    % Distance
    \draw[dashed] (0,0) -- (2,1.5);
    \node at (1,0.5) {\tiny 1.5m};
\end{tikzpicture}
\end{center}

\textbf{Output Coordinates:}
\begin{itemize}
    \item X-axis: Forward (depth from camera)
    \item Y-axis: Horizontal (left-right)
    \item Z-axis: Vertical (up-down)
\end{itemize}
\end{frame}

% Section 8: Troubleshooting
\section{Troubleshooting}

\begin{frame}{Common Issues and Solutions}
\begin{table}
\scriptsize
\begin{tabular}{|p{4cm}|p{5.5cm}|}
\hline
\textbf{Problem} & \textbf{Solution} \\
\hline
No 3D boxes appearing & Check point cloud topic is publishing\\
& Verify object in \texttt{interested\_classes}\\
& Lower \texttt{minimum\_probability} \\
\hline
TF transform errors & Ensure camera driver publishes TF\\
& Verify \texttt{working\_frame} name\\
\hline
High CPU usage & Use GPU acceleration\\
& Switch to YOLOv3-tiny\\
& Reduce camera resolution\\
\hline
Inaccurate 3D bounds & Tune \texttt{max\_detection\_threshold}\\
& Check depth sensor calibration\\
\hline
\end{tabular}
\end{table}
\end{frame}

\begin{frame}[fragile]{Performance Optimization}
\textbf{1. Enable GPU Acceleration:}
\begin{itemize}
    \item Install CUDA toolkit
    \item Rebuild darknet with GPU support
    \item 5-10x speed improvement
\end{itemize}

\vspace{0.5em}
\textbf{2. Optimize Parameters:}
\begin{lstlisting}[language=yaml, basicstyle=\tiny\ttfamily]
# Reduce classes for faster processing
interested_classes: ["person"]  # Only detect people

# Increase threshold to skip low-confidence
minimum_probability: 0.5

# Reduce camera resolution
# In camera driver: width: 640, height: 480
\end{lstlisting}

\vspace{0.5em}
\textbf{3. Use Lightweight Model:}
\begin{itemize}
    \item YOLOv3-tiny: 2-3x faster than YOLOv3
    \item Trade-off: slightly lower accuracy
\end{itemize}
\end{frame}

% Section 9: Future Work
\section{Conclusion and Future Work}

\begin{frame}{Project Achievements}
\textbf{Successfully Implemented:}
\begin{itemize}
    \item ✓ Real-time 2D object detection (YOLO)
    \item ✓ 3D bounding box calculation from point clouds
    \item ✓ ROS 2 Humble integration
    \item ✓ Multi-object detection (80+ classes)
    \item ✓ RViz visualization
    \item ✓ Configurable parameters
    \item ✓ Complete documentation
    \item ✓ Open-source on GitHub
\end{itemize}

\vspace{1em}
\textbf{Repository:}\\
\url{https://github.com/HUOJIAXI/3D_bounding_box_gb_visual_detection}
\end{frame}

\begin{frame}{Future Improvements}
\begin{columns}
\column{0.5\textwidth}
\textbf{Short-term:}
\begin{itemize}
    \item Object tracking over time
    \item Kalman filtering for stability
    \item Multiple camera support
    \item Dynamic reconfigure
\end{itemize}

\vspace{1em}
\textbf{Long-term:}
\begin{itemize}
    \item Oriented bounding boxes
    \item Instance segmentation
    \item Object pose estimation
    \item Deep learning for 3D
\end{itemize}

\column{0.5\textwidth}
\textbf{Integration Ideas:}
\begin{itemize}
    \item Robot arm grasping
    \item Autonomous navigation
    \item AR/VR applications
    \item Warehouse automation
    \item Human activity recognition
\end{itemize}

\vspace{1em}
\textbf{Performance:}
\begin{itemize}
    \item Optimize for embedded systems
    \item TensorRT acceleration
    \item Multi-threaded processing
\end{itemize}
\end{columns}
\end{frame}

\begin{frame}{Key Takeaways}
\begin{enumerate}
    \item \textbf{Modular Design:}
    \begin{itemize}
        \item 2D detection (darknet\_ros)
        \item 3D projection (gb\_visual\_detection\_3d)
        \item Easy to extend and modify
    \end{itemize}

    \item \textbf{ROS 2 Native:}
    \begin{itemize}
        \item Standard message types
        \item Easy integration with existing systems
        \item Lifecycle management
    \end{itemize}

    \item \textbf{Real-time Capable:}
    \begin{itemize}
        \item 20-30 FPS with GPU
        \item Low latency ($<$50ms total)
        \item Suitable for robotic applications
    \end{itemize}

    \item \textbf{Well-Documented:}
    \begin{itemize}
        \item Comprehensive README
        \item Configuration examples
        \item Troubleshooting guide
    \end{itemize}
\end{enumerate}
\end{frame}

% References
\begin{frame}{References and Resources}
\begin{itemize}
    \item \textbf{darknet\_ros:}\\
    \url{https://github.com/leggedrobotics/darknet_ros}

    \item \textbf{gb\_visual\_detection\_3d:}\\
    \url{https://github.com/IntelligentRoboticsLabs/gb_visual_detection_3d}

    \item \textbf{YOLO (Darknet):}\\
    \url{https://pjreddie.com/darknet/yolo/}

    \item \textbf{ROS 2 Humble:}\\
    \url{https://docs.ros.org/en/humble/}

    \item \textbf{This Project:}\\
    \url{https://github.com/HUOJIAXI/3D_bounding_box_gb_visual_detection}
\end{itemize}
\end{frame}

% Thank you slide
\begin{frame}
\begin{center}
{\Huge Thank You!}

\vspace{2em}

{\Large Questions?}

\vspace{3em}

\textbf{Contact:}\\
HUOJIAXI\\
JasonHuoeclille@gmail.com\\

\vspace{1em}

\textbf{GitHub:}\\
\url{https://github.com/HUOJIAXI/3D_bounding_box_gb_visual_detection}
\end{center}
\end{frame}

\end{document}
